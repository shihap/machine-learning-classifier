# -*- coding: utf-8 -*-
"""assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wpslaPPNdTn-trljAaJz48lP7fLm7qu1

#mount google drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""#step 0 - declaring"""

#made by shehab ahmed mohamed 20170129 & abd elrahman bahig 20170143 
import numpy
from numpy import loadtxt
from keras.models import Sequential
from keras.layers import Dense
import warnings
import matplotlib.pyplot as plt
import os
import nltk as nltk
from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer
import pandas as pd
import random
import string
import re
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn import svm
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
lemmatizer = WordNetLemmatizer()
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

warnings.filterwarnings("ignore")

#new
import gensim
from gensim.test.utils import common_texts
from gensim.models import Word2Vec


# lemmatize string
def lemmatize_word(text):
    word_tokens = word_tokenize(text)
    # provide context i.e. part-of-speech
    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]
    return lemmas
  
# remove stopwords function
def remove_stopwords(text):
    stop_words = set(stopwords.words("english"))
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word not in stop_words]
    return filtered_text


stemmer = PorterStemmer()


# stem words in the list of tokenised words
def stem_words(text):
    word_tokens = word_tokenize(text)
    stems = [stemmer.stem(word) for word in word_tokens]
    return stems


# Read text File
def read_text_file(file_path):
    with open(file_path, 'r') as f: 
        arr.append(f.read())


#get the tf-idf from input
def convert_input(string):

    #1-no new words , filter array of unique words the {other dataframe} d2
    #a-string -> array (split)
    array = string.split(" ")
    #b-array -> new array (for loop , in)
    new_array = []
    for a in array:
        if a in df0:
            new_array.append(a)
    #c-new array -> new string (join)
    new_string = ' '.join(new_array)
    print("new_string:"+new_string)

    #2-push it to the data files array (append)
    arr.append(new_string)

    #3-calculate the tf idf again and get the last element
    tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(arr)
    
    first_vector_tfidfvectorizer = tfidf_vectorizer_vectors[len(arr)-1] #the last element
    t = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=["tfidf"])
    t.sort_values(by=["tfidf"], ascending=False)

    t1 = t["tfidf"].tolist()

    #4-delete the last element from the array (pop)
    arr.pop()

    #5-return (the last)
    return t1



def tokenize_file(f):#input : review or file (string)

    w2v_data.append(f.split(" "))#add array of words of the file in word_2_vector negative
                  


def sentence_embedding(sent,model):
    #init empty array
    a = []
    
    #append zeros with size times
    for i in range(model.vector_size):
        a.append(0)
        
    word_count = 0
    #sum
    for word in sent:
        a = [x + y for x, y in zip(a, model.wv[word])]
        word_count+=1

    #div
    for i in range(model.vector_size):
        a[i]/=word_count
        
    #return array
    return a

"""#step 1 - get the files into arr"""

#change it
num_of_files = 2000


# Folder Path
from typing import List

dir = [x[0] for x in os.walk("/content/drive/MyDrive/Colab Notebooks/txt_sentoken")]


arr = []
arr1 = []
arr2 = []

# iterate through all file
for d in dir:
    # Change the directory
    os.chdir(d)

    count = 0
    for file in os.listdir():
        
        if count == num_of_files/2: 
            break
        print("file:",count+1)
        count = count + 1

        # Check whether file is in text format or not
        if file.endswith(".txt"):
            file_path = f"{d}/{file}"

            # call read text file function

            read_text_file(file_path)

"""#for tf-idf pre-proccesing"""

arr1 = []
for i in range(len(arr)):
        print("tf-idf pre-processing:",i+1)
        temp1 = re.sub(r'\d+', '', arr[i]) #remove numbers
        temp1=" ".join(re.sub(r'[^\w]', ' ', temp1).split())#remove symbols and whitespaces
        temp1 = " ".join(remove_stopwords(temp1)) #stopwords
        temp1 = " ".join(lemmatize_word(temp1)) #lemmatize
        temp1 = " ".join(stem_words(temp1)) #stem     
        arr1.append(temp1)

"""#for word2vec pre-proccesing"""

arr2 = []
for i in range(len(arr)):
        print("word2vec pre-processing:",i+1)
        temp2 = re.sub(r'\W', ' ', arr[i]) #remove symbols
        temp2 = re.sub(r'[0-9]', ' ', temp2) #numbers
        temp2 = re.sub(r'\s+[a-zA-Z]\s+', ' ', temp2) #remove single characters
        temp2 = re.sub(r'\s+', ' ', temp2, flags=re.I) #whitespace
        temp2 = temp2.lower() #lower
        #temp2 = " ".join(remove_stopwords(temp2)) #stopwords 
        arr2.append(temp2)

"""#tf-idf data"""

tfidf_vectorizer = TfidfVectorizer(use_idf=True)

# just send in all your docs here
tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(arr1)

# get the first vector out (for the first document)
data = []
for i in range(num_of_files):
    if i % 100 == 0:
        print(i)
    first_vector_tfidfvectorizer = tfidf_vectorizer_vectors[i]
    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=["tfidf"])
    df.sort_values(by=["tfidf"], ascending=False)

    df1 = df["tfidf"].tolist()
    data.append(df1)

"""#word2vec data"""

w2v_data = []


#call the function
for i in range(len(arr2)):
        tokenize_file(arr2[i])

# Create CBOW model
CBOW_model = gensim.models.Word2Vec(w2v_data , min_count = 1 , window = 3 , size = 100 , sg = 0 , iter = 10)


data2 = [sentence_embedding(i,CBOW_model) for i in w2v_data]

"""#training and testing data"""

y = []
for i in range(num_of_files):
  if i < num_of_files/2:
    y.append(0)
  else:
    y.append(1)


x_train, x_test, y_train, y_test = train_test_split(data, y, test_size = 0.20)

x_train2, x_test2, y_train2, y_test2 = train_test_split(data2, y, test_size = 0.20)

"""#training SVM"""

clf = LinearSVC(random_state=0, tol=1e-5)
clf.fit(x_train, y_train)

clf2 = LinearSVC(random_state=0, tol=1e-5)
clf2.fit(x_train2, y_train2)

"""#training CNN"""

# define the keras model / model 1
model = Sequential()
model.add(Dense(8, input_dim=len(x_train[0]), activation='relu')) #len(x_train[0])=number of unique words , fixed length
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))


# define the keras model / model 2
model2 = Sequential()
model2.add(Dense(8, input_dim=len(x_train2[0]), activation='relu')) #len(x_train[0])=number of unique words , fixed length
model2.add(Dense(10, activation='relu'))
model2.add(Dense(1, activation='sigmoid'))

# compile the keras model / model 1
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# compile the keras model / model 2
model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

print("wait please , training model 1 (tf-idf/CNN)!!!!!") #/ model 1
# fit the keras model on the dataset (training) -> change to 20 and 150 , verbose=0
model.fit(x_train, y_train, epochs=1, batch_size=10)

print("wait please , training model 2 (word2vector/CNN)!!!!!") #/ model 2
# fit the keras model on the dataset (training) -> change to 20 and 150 , verbose=0
model2.fit(x_train2, y_train2, epochs=1, batch_size=10)

"""#testing SVM"""

predictions_SVM = clf.predict(x_test)
# Use accuracy_score function to get the accuracy
print("SVM Accuracy Score -> ",accuracy_score(predictions_SVM, y_test)*100)

predictions_SVM2 = clf2.predict(x_test2)
# Use accuracy_score function to get the accuracy
print("SVM2 Accuracy Score -> ",accuracy_score(predictions_SVM2, y_test2)*100)

"""#testing CNN"""

print("wait please , testing model 1 !!!!!") #/ model 1
# evaluate the keras model (testing) 
_, accuracy = model.evaluate(x_test, y_test)
print('Accuracy: %.2f' % (accuracy*100))


print("wait please , testing model 2 !!!!!") #/ model 2
# evaluate the keras model (testing) 
_, accuracy2 = model2.evaluate(x_test2, y_test2)
print('Accuracy2: %.2f' % (accuracy2*100))

#'''
# ignore warning
warnings.filterwarnings("ignore")

print("wait please , making predictions !!!!!") #/ model 1
# make class predictions with the model
predictions = model.predict_classes(x_test)


print("wait please , making predictions !!!!!") #/ model 2
# make class predictions with the model
predictions2 = model2.predict_classes(x_test2)

# summarize the first 5 cases / model 1
for i in range(10):
	print(' => %d (expected %d)' % (predictions[i], y_test[i]))


# summarize the first 5 cases / model 2
for i in range(10):
	print(' => %d (expected2 %d)' % (predictions2[i], y_test2[i]))

#'''

"""#playground(use it to debug)"""

